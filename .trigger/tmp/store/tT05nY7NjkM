{
  "version": 3,
  "sources": ["../../../../../../trigger/discover-research.ts"],
  "sourcesContent": ["import { schedules, logger, metadata } from \"@trigger.dev/sdk\";\nimport { createClient } from \"@supabase/supabase-js\";\n\nconst supabase = createClient(\n    process.env.SUPABASE_URL!,\n    process.env.SUPABASE_SERVICE_ROLE_KEY!\n);\n\n// ‚îÄ‚îÄ Types ‚îÄ‚îÄ\ninterface BrandKitRow {\n    user_id: string;\n    kit_name: string;\n    org_type: string;\n    org_level: string;\n    office_sought: string;\n    state: string;\n    brand_summary: string;\n}\n\ninterface StanceRow {\n    user_id: string;\n    stances: { issue: string; position: string; priority: string }[];\n}\n\ninterface SonarCitation {\n    url: string;\n    title?: string;\n}\n\ninterface SonarChoice {\n    message: {\n        content: string;\n    };\n}\n\ninterface SonarResponse {\n    choices: SonarChoice[];\n    citations?: string[] | SonarCitation[];\n}\n\ninterface ScoredTopic {\n    title: string;\n    summary: string;\n    source_url: string;\n    source_domain: string;\n    content_snippet: string;\n    relevance_score: number;\n}\n\n// ‚îÄ‚îÄ Tier-1 & Tier-2 news sources ‚îÄ‚îÄ\nconst TIER_1 = [\"nytimes.com\", \"washingtonpost.com\", \"reuters.com\", \"apnews.com\"];\nconst TIER_2 = [\"politico.com\", \"thehill.com\", \"cnn.com\", \"npr.org\", \"axios.com\", \"fec.gov\", \"opensecrets.org\", \"nbcnews.com\"];\n\n/**\n * Discover Research ‚Äî Scheduled Task (every 4 hours)\n *\n * For each user with a brand kit:\n * 1. Build a personalized search query from their stances + org info\n * 2. Call Perplexity Sonar API\n * 3. Score results (recency, source quality, relevance)\n * 4. Save top results to research_topics with suggested_by='ai'\n * 5. Deduplicate against existing topics\n */\nexport const discoverResearch = schedules.task({\n    id: \"discover-research\",\n    machine: { preset: \"small-2x\" },\n    retry: {\n        maxAttempts: 2,\n        factor: 1.5,\n        minTimeoutInMs: 5000,\n        maxTimeoutInMs: 30_000,\n    },\n    run: async () => {\n        logger.info(\"üîç Discover Research ‚Äî scheduled run starting\");\n        metadata.set(\"status\", \"fetching_users\");\n\n        // 1. Fetch all brand kits\n        const { data: brandKits, error: bkError } = await supabase\n            .from(\"brand_kits\")\n            .select(\"user_id, kit_name, org_type, org_level, office_sought, state, brand_summary\");\n\n        if (bkError || !brandKits || brandKits.length === 0) {\n            logger.info(\"No brand kits found, skipping\", { error: bkError?.message });\n            return { usersProcessed: 0, totalSaved: 0 };\n        }\n\n        logger.info(`Found ${brandKits.length} brand kits to process`);\n        metadata.set(\"totalUsers\", brandKits.length);\n\n        let totalSaved = 0;\n        let usersProcessed = 0;\n\n        for (const kit of brandKits as BrandKitRow[]) {\n            try {\n                const saved = await processUserDiscovery(kit);\n                totalSaved += saved;\n                usersProcessed++;\n                metadata.set(\"usersProcessed\", usersProcessed).set(\"totalSaved\", totalSaved);\n            } catch (err) {\n                logger.error(`Failed to process user ${kit.user_id}`, {\n                    error: (err as Error).message,\n                });\n            }\n        }\n\n        metadata.set(\"status\", \"completed\");\n        logger.info(\"üéâ Discover Research complete\", { usersProcessed, totalSaved });\n\n        return { usersProcessed, totalSaved };\n    },\n});\n\n/**\n * Process discovery for a single user\n */\nasync function processUserDiscovery(kit: BrandKitRow): Promise<number> {\n    const query = buildDiscoveryQuery(kit);\n    if (!query) {\n        logger.info(`Skipping user ${kit.user_id} ‚Äî no stances or context to search`);\n        return 0;\n    }\n\n    logger.info(`Searching for user ${kit.user_id}`, { query: query.slice(0, 100) });\n\n    // Fetch existing topic URLs for this user to deduplicate\n    const { data: existing } = await supabase\n        .from(\"research_topics\")\n        .select(\"source_url\")\n        .eq(\"user_id\", kit.user_id)\n        .order(\"created_at\", { ascending: false })\n        .limit(100);\n\n    const existingUrls = new Set((existing || []).map((t: { source_url: string }) => t.source_url));\n\n    // Call Perplexity Sonar\n    const topics = await sonarSearch(query);\n\n    if (topics.length === 0) {\n        logger.info(`No results for user ${kit.user_id}`);\n        return 0;\n    }\n\n    // Deduplicate and save top 5\n    const newTopics = topics\n        .filter((t) => !existingUrls.has(t.source_url))\n        .slice(0, 5);\n\n    let saved = 0;\n    for (const topic of newTopics) {\n        const { error } = await supabase.from(\"research_topics\").insert({\n            user_id: kit.user_id,\n            title: topic.title,\n            summary: topic.summary,\n            source_url: topic.source_url,\n            source_domain: topic.source_domain,\n            content_snippet: topic.content_snippet,\n            relevance_score: topic.relevance_score,\n            suggested_by: \"ai\",\n            used_in_draft: false,\n        });\n\n        if (!error) saved++;\n        else logger.error(\"Insert failed\", { title: topic.title, error: error.message });\n    }\n\n    logger.info(`Saved ${saved} topics for user ${kit.user_id}`);\n    return saved;\n}\n\n/**\n * Build a multi-angle discovery prompt from user's brand kit.\n * Generates multiple query fragments and combines them into a single\n * structured Sonar prompt for comprehensive coverage.\n *\n * Query formats:\n * 1. org_name\n * 2. org_name + office_sought\n * 3. district + state + office_sought\n * 4. office_sought + state\n * 6. stance_issue + office_sought\n * 7. stance_issue + stance_position\n * 8. office_sought + state + \"election\"\n * 9. office_sought + district + \"fundraising\"\n * 14. org_level + state\n */\nfunction buildDiscoveryQuery(kit: BrandKitRow): string | null {\n    const queries: string[] = [];\n    const name = kit.kit_name?.trim();\n    const office = kit.office_sought?.trim();\n    const state = kit.state?.trim();\n    const district = kit.district?.trim();\n    const level = kit.org_level?.trim(); // Federal, State, Local\n    const summary = kit.brand_summary?.trim();\n\n    // Extract stance-like keywords from brand summary\n    const stanceKeywords = extractStanceKeywords(summary);\n\n    // ‚îÄ‚îÄ Format 1: org_name ‚îÄ‚îÄ\n    if (name) {\n        queries.push(name);\n    }\n\n    // ‚îÄ‚îÄ Format 2: org_name + office_sought ‚îÄ‚îÄ\n    if (name && office) {\n        queries.push(`${name} ${office} race`);\n    }\n\n    // ‚îÄ‚îÄ Format 3: district + state + office_sought ‚îÄ‚îÄ\n    if (district && state && office) {\n        queries.push(`${district} ${state} ${office}`);\n    }\n\n    // ‚îÄ‚îÄ Format 4: office_sought + state ‚îÄ‚îÄ\n    if (office && state) {\n        queries.push(`${office} ${state}`);\n    }\n\n    // ‚îÄ‚îÄ Format 6: stance_issue + office_sought ‚îÄ‚îÄ\n    if (stanceKeywords.length > 0 && office) {\n        // Pick top 2 stance keywords\n        for (const keyword of stanceKeywords.slice(0, 2)) {\n            queries.push(`${keyword} ${office}`);\n        }\n    }\n\n    // ‚îÄ‚îÄ Format 7: stance_issue + stance_position ‚îÄ‚îÄ\n    if (stanceKeywords.length > 0) {\n        // Use the first stance keyword with context\n        queries.push(stanceKeywords[0]);\n    }\n\n    // ‚îÄ‚îÄ Format 8: office_sought + state + \"election\" ‚îÄ‚îÄ\n    if (office && state) {\n        queries.push(`${office} ${state} election 2026`);\n    }\n\n    // ‚îÄ‚îÄ Format 9: office_sought + district + \"fundraising\" ‚îÄ‚îÄ\n    if (office && district) {\n        queries.push(`${office} ${district} fundraising`);\n    }\n\n    // ‚îÄ‚îÄ Format 14: org_level + state ‚îÄ‚îÄ\n    if (level && state) {\n        queries.push(`${level} races ${state}`);\n    }\n\n    // If no queries could be built, skip\n    if (queries.length === 0) return null;\n\n    // Deduplicate\n    const unique = [...new Set(queries)];\n\n    // Combine into a natural language prompt (Sonar handles this better than numbered lists)\n    const topicList = unique.join(\", \");\n    return `Search for recent news about: ${topicList}. Find political news, campaign updates, fundraising developments, election coverage, and policy stories relevant to these topics. Return diverse results from different angles.`;\n}\n\n/**\n * Extract stance-like keywords from the brand summary.\n * Looks for common policy areas mentioned in the text.\n */\nfunction extractStanceKeywords(summary: string | undefined): string[] {\n    if (!summary) return [];\n\n    const policyTerms = [\n        \"healthcare\", \"health care\", \"medicare\", \"medicaid\",\n        \"climate\", \"environment\", \"clean energy\", \"green\",\n        \"education\", \"schools\", \"student\",\n        \"immigration\", \"border\",\n        \"economy\", \"jobs\", \"inflation\", \"wages\",\n        \"gun\", \"firearms\", \"second amendment\",\n        \"abortion\", \"reproductive\", \"women's health\",\n        \"housing\", \"rent\", \"affordable housing\",\n        \"criminal justice\", \"police\", \"public safety\",\n        \"veterans\", \"military\", \"defense\",\n        \"taxes\", \"tax reform\",\n        \"voting rights\", \"election integrity\",\n        \"infrastructure\", \"transportation\",\n        \"social security\", \"retirement\",\n    ];\n\n    const lower = summary.toLowerCase();\n    return policyTerms.filter((term) => lower.includes(term));\n}\n\n/**\n * Call Perplexity Sonar API and extract REAL citations from the response.\n *\n * Key insight: Sonar's `citations` array contains verified URLs.\n * The LLM content references them with [1], [2] markers.\n * We parse the content into topics and map citations to real URLs.\n */\nasync function sonarSearch(query: string): Promise<ScoredTopic[]> {\n    const apiKey = process.env.PERPLEXITY_API_KEY;\n    if (!apiKey) {\n        logger.error(\"PERPLEXITY_API_KEY not set\");\n        return [];\n    }\n\n    try {\n        const response = await fetch(\"https://api.perplexity.ai/chat/completions\", {\n            method: \"POST\",\n            headers: {\n                \"Content-Type\": \"application/json\",\n                Authorization: `Bearer ${apiKey}`,\n            },\n            body: JSON.stringify({\n                model: \"sonar\",\n                messages: [\n                    {\n                        role: \"system\",\n                        content:\n                            \"You are a political news researcher. Write a numbered list of 5-10 recent, distinct news stories. For each item write: the headline, a 1-2 sentence summary, and cite your source using [number] notation matching the citations provided. Example format:\\n1. **Headline here** ‚Äî Summary of the story. [1]\\n2. **Another headline** ‚Äî Summary text. [3]\",\n                    },\n                    {\n                        role: \"user\",\n                        content: query,\n                    },\n                ],\n                temperature: 0.2,\n                max_tokens: 2500,\n                search_recency_filter: \"day\",\n            }),\n        });\n\n        if (!response.ok) {\n            const errorText = await response.text();\n            logger.error(\"Sonar API error\", { status: response.status, body: errorText });\n            return [];\n        }\n\n        const data = (await response.json()) as SonarResponse;\n        const content = data.choices?.[0]?.message?.content || \"\";\n\n        // Extract REAL citations from the API response (not from LLM text)\n        const citations: string[] = [];\n        if (Array.isArray(data.citations)) {\n            for (const c of data.citations) {\n                if (typeof c === \"string\") citations.push(c);\n                else if (c && typeof c === \"object\" && \"url\" in c) citations.push(c.url);\n            }\n        }\n\n        logger.info(\"Sonar response\", {\n            contentLength: content.length,\n            citationCount: citations.length,\n            sampleCitations: citations.slice(0, 3),\n        });\n\n        if (citations.length === 0) {\n            logger.warn(\"No citations in Sonar response ‚Äî cannot produce verified results\");\n            return [];\n        }\n\n        // Parse numbered items from the content\n        const topics = parseNumberedList(content, citations);\n\n        // Score and sort\n        return topics\n            .map((topic) => ({\n                ...topic,\n                relevance_score: scoreResult(topic.title, topic.summary, topic.source_domain, \"\"),\n            }))\n            .sort((a, b) => b.relevance_score - a.relevance_score);\n    } catch (err) {\n        logger.error(\"Sonar search failed\", { error: (err as Error).message });\n        return [];\n    }\n}\n\n/**\n * Parse numbered list items from Sonar content and map citation markers to real URLs.\n *\n * Input format:  \"1. **Headline** ‚Äî Summary [1][3]\\n2. **Another** ‚Äî Text [2]\"\n * Citations:     [\"https://real-url-1.com\", \"https://real-url-2.com\", \"https://real-url-3.com\"]\n */\nfunction parseNumberedList(content: string, citations: string[]): Omit<ScoredTopic, \"relevance_score\">[] {\n    const results: Omit<ScoredTopic, \"relevance_score\">[] = [];\n\n    // Split by numbered items: \"1. \", \"2. \", etc.\n    const items = content.split(/\\n?\\d+\\.\\s+/).filter((s) => s.trim().length > 10);\n\n    for (const item of items) {\n        // Extract title: look for **bold** text or first sentence\n        let title = \"\";\n        let summary = item.trim();\n\n        const boldMatch = item.match(/\\*\\*(.+?)\\*\\*/);\n        if (boldMatch) {\n            title = boldMatch[1].trim();\n            // Summary is everything after the title, cleaned up\n            summary = item\n                .replace(/\\*\\*(.+?)\\*\\*/, \"\")\n                .replace(/^\\s*[‚Äî‚Äì-]\\s*/, \"\")\n                .trim();\n        } else {\n            // Use first sentence as title\n            const firstSentence = item.split(/[.!?]/)[0];\n            title = firstSentence?.trim() || item.slice(0, 80);\n            summary = item.slice(title.length).trim();\n        }\n\n        // Extract citation numbers [1], [2], [3] from the item\n        const citationMatches = item.match(/\\[(\\d+)\\]/g);\n        let sourceUrl = \"\";\n\n        if (citationMatches && citationMatches.length > 0) {\n            // Use the first citation number, convert to 0-indexed\n            const citNum = parseInt(citationMatches[0].replace(/[\\[\\]]/g, \"\"), 10) - 1;\n            if (citNum >= 0 && citNum < citations.length) {\n                sourceUrl = citations[citNum];\n            }\n        }\n\n        // Clean up summary: remove citation markers\n        summary = summary.replace(/\\[\\d+\\]/g, \"\").trim();\n        // Remove trailing dashes/colons\n        summary = summary.replace(/^[‚Äî‚Äì:\\s]+/, \"\").replace(/[‚Äî‚Äì:\\s]+$/, \"\");\n\n        if (!title || title.length < 5) continue;\n        if (!sourceUrl) continue; // Skip items without a verified citation\n\n        const domain = extractDomain(sourceUrl);\n\n        results.push({\n            title: title.slice(0, 200),\n            summary: summary.slice(0, 500) || title,\n            source_url: sourceUrl,\n            source_domain: domain,\n            content_snippet: summary.slice(0, 300) || title,\n        });\n    }\n\n    return results;\n}\n\n/**\n * Score a result on a fixed 0-10 scale.\n * Source quality is the primary signal ‚Äî Sonar already filtered for relevance.\n */\nfunction scoreResult(title: string, summary: string, domain: string, _query: string): number {\n    let score = 2; // Base score: Sonar returned it, so it's at least somewhat relevant\n\n    // Source quality (0-5 points) ‚Äî biggest differentiator\n    if (TIER_1.some((d) => domain.includes(d))) score += 5;\n    else if (TIER_2.some((d) => domain.includes(d))) score += 3;\n    else if (domain) score += 1; // Known domain but not in our tiers\n\n    // Content richness bonus (0-2 points)\n    if (title.length > 30) score += 0.5;\n    if (summary.length > 100) score += 0.5;\n    if (summary.length > 200) score += 0.5;\n    if (title.length > 50 && summary.length > 150) score += 0.5;\n\n    // Cap at 10\n    return Math.min(Math.round(score * 100) / 100, 10);\n}\n\n/**\n * Parse a JSON array from potentially messy LLM output\n */\nfunction parseJsonArray(content: string): Record<string, string>[] | null {\n    try {\n        // Try direct parse first\n        const parsed = JSON.parse(content);\n        if (Array.isArray(parsed)) return parsed;\n    } catch {\n        // Try extracting JSON from markdown code blocks\n        const match = content.match(/\\[[\\s\\S]*\\]/);\n        if (match) {\n            try {\n                return JSON.parse(match[0]);\n            } catch {\n                return null;\n            }\n        }\n    }\n    return null;\n}\n\nfunction extractDomain(url: string): string {\n    try {\n        return new URL(url).hostname.replace(\"www.\", \"\");\n    } catch {\n        return \"\";\n    }\n}\n"],
  "mappings": ";;;;;;;;;;;;;;;;AAAA;AAGA,IAAM,WAAW;AAAA,EACb,QAAQ,IAAI;AAAA,EACZ,QAAQ,IAAI;AAChB;AA4CA,IAAM,SAAS,CAAC,eAAe,sBAAsB,eAAe,YAAY;AAChF,IAAM,SAAS,CAAC,gBAAgB,eAAe,WAAW,WAAW,aAAa,WAAW,mBAAmB,aAAa;AAYtH,IAAM,mBAAmB,kBAAU,KAAK;AAAA,EAC3C,IAAI;AAAA,EACJ,SAAS,EAAE,QAAQ,WAAW;AAAA,EAC9B,OAAO;AAAA,IACH,aAAa;AAAA,IACb,QAAQ;AAAA,IACR,gBAAgB;AAAA,IAChB,gBAAgB;AAAA,EACpB;AAAA,EACA,KAAK,mCAAY;AACb,WAAO,KAAK,+CAA+C;AAC3D,aAAS,IAAI,UAAU,gBAAgB;AAGvC,UAAM,EAAE,MAAM,WAAW,OAAO,QAAQ,IAAI,MAAM,SAC7C,KAAK,YAAY,EACjB,OAAO,6EAA6E;AAEzF,QAAI,WAAW,CAAC,aAAa,UAAU,WAAW,GAAG;AACjD,aAAO,KAAK,iCAAiC,EAAE,OAAO,SAAS,QAAQ,CAAC;AACxE,aAAO,EAAE,gBAAgB,GAAG,YAAY,EAAE;AAAA,IAC9C;AAEA,WAAO,KAAK,SAAS,UAAU,MAAM,wBAAwB;AAC7D,aAAS,IAAI,cAAc,UAAU,MAAM;AAE3C,QAAI,aAAa;AACjB,QAAI,iBAAiB;AAErB,eAAW,OAAO,WAA4B;AAC1C,UAAI;AACA,cAAM,QAAQ,MAAM,qBAAqB,GAAG;AAC5C,sBAAc;AACd;AACA,iBAAS,IAAI,kBAAkB,cAAc,EAAE,IAAI,cAAc,UAAU;AAAA,MAC/E,SAAS,KAAK;AACV,eAAO,MAAM,0BAA0B,IAAI,OAAO,IAAI;AAAA,UAClD,OAAQ,IAAc;AAAA,QAC1B,CAAC;AAAA,MACL;AAAA,IACJ;AAEA,aAAS,IAAI,UAAU,WAAW;AAClC,WAAO,KAAK,iCAAiC,EAAE,gBAAgB,WAAW,CAAC;AAE3E,WAAO,EAAE,gBAAgB,WAAW;AAAA,EACxC,GArCK;AAsCT,CAAC;AAKD,eAAe,qBAAqB,KAAmC;AACnE,QAAM,QAAQ,oBAAoB,GAAG;AACrC,MAAI,CAAC,OAAO;AACR,WAAO,KAAK,iBAAiB,IAAI,OAAO,oCAAoC;AAC5E,WAAO;AAAA,EACX;AAEA,SAAO,KAAK,sBAAsB,IAAI,OAAO,IAAI,EAAE,OAAO,MAAM,MAAM,GAAG,GAAG,EAAE,CAAC;AAG/E,QAAM,EAAE,MAAM,SAAS,IAAI,MAAM,SAC5B,KAAK,iBAAiB,EACtB,OAAO,YAAY,EACnB,GAAG,WAAW,IAAI,OAAO,EACzB,MAAM,cAAc,EAAE,WAAW,MAAM,CAAC,EACxC,MAAM,GAAG;AAEd,QAAM,eAAe,IAAI,KAAK,YAAY,CAAC,GAAG,IAAI,CAAC,MAA8B,EAAE,UAAU,CAAC;AAG9F,QAAM,SAAS,MAAM,YAAY,KAAK;AAEtC,MAAI,OAAO,WAAW,GAAG;AACrB,WAAO,KAAK,uBAAuB,IAAI,OAAO,EAAE;AAChD,WAAO;AAAA,EACX;AAGA,QAAM,YAAY,OACb,OAAO,CAAC,MAAM,CAAC,aAAa,IAAI,EAAE,UAAU,CAAC,EAC7C,MAAM,GAAG,CAAC;AAEf,MAAI,QAAQ;AACZ,aAAW,SAAS,WAAW;AAC3B,UAAM,EAAE,MAAM,IAAI,MAAM,SAAS,KAAK,iBAAiB,EAAE,OAAO;AAAA,MAC5D,SAAS,IAAI;AAAA,MACb,OAAO,MAAM;AAAA,MACb,SAAS,MAAM;AAAA,MACf,YAAY,MAAM;AAAA,MAClB,eAAe,MAAM;AAAA,MACrB,iBAAiB,MAAM;AAAA,MACvB,iBAAiB,MAAM;AAAA,MACvB,cAAc;AAAA,MACd,eAAe;AAAA,IACnB,CAAC;AAED,QAAI,CAAC,MAAO;AAAA,QACP,QAAO,MAAM,iBAAiB,EAAE,OAAO,MAAM,OAAO,OAAO,MAAM,QAAQ,CAAC;AAAA,EACnF;AAEA,SAAO,KAAK,SAAS,KAAK,oBAAoB,IAAI,OAAO,EAAE;AAC3D,SAAO;AACX;AApDe;AAsEf,SAAS,oBAAoB,KAAiC;AAC1D,QAAM,UAAoB,CAAC;AAC3B,QAAM,OAAO,IAAI,UAAU,KAAK;AAChC,QAAM,SAAS,IAAI,eAAe,KAAK;AACvC,QAAM,QAAQ,IAAI,OAAO,KAAK;AAC9B,QAAM,WAAW,IAAI,UAAU,KAAK;AACpC,QAAM,QAAQ,IAAI,WAAW,KAAK;AAClC,QAAM,UAAU,IAAI,eAAe,KAAK;AAGxC,QAAM,iBAAiB,sBAAsB,OAAO;AAGpD,MAAI,MAAM;AACN,YAAQ,KAAK,IAAI;AAAA,EACrB;AAGA,MAAI,QAAQ,QAAQ;AAChB,YAAQ,KAAK,GAAG,IAAI,IAAI,MAAM,OAAO;AAAA,EACzC;AAGA,MAAI,YAAY,SAAS,QAAQ;AAC7B,YAAQ,KAAK,GAAG,QAAQ,IAAI,KAAK,IAAI,MAAM,EAAE;AAAA,EACjD;AAGA,MAAI,UAAU,OAAO;AACjB,YAAQ,KAAK,GAAG,MAAM,IAAI,KAAK,EAAE;AAAA,EACrC;AAGA,MAAI,eAAe,SAAS,KAAK,QAAQ;AAErC,eAAW,WAAW,eAAe,MAAM,GAAG,CAAC,GAAG;AAC9C,cAAQ,KAAK,GAAG,OAAO,IAAI,MAAM,EAAE;AAAA,IACvC;AAAA,EACJ;AAGA,MAAI,eAAe,SAAS,GAAG;AAE3B,YAAQ,KAAK,eAAe,CAAC,CAAC;AAAA,EAClC;AAGA,MAAI,UAAU,OAAO;AACjB,YAAQ,KAAK,GAAG,MAAM,IAAI,KAAK,gBAAgB;AAAA,EACnD;AAGA,MAAI,UAAU,UAAU;AACpB,YAAQ,KAAK,GAAG,MAAM,IAAI,QAAQ,cAAc;AAAA,EACpD;AAGA,MAAI,SAAS,OAAO;AAChB,YAAQ,KAAK,GAAG,KAAK,UAAU,KAAK,EAAE;AAAA,EAC1C;AAGA,MAAI,QAAQ,WAAW,EAAG,QAAO;AAGjC,QAAM,SAAS,CAAC,GAAG,IAAI,IAAI,OAAO,CAAC;AAGnC,QAAM,YAAY,OAAO,KAAK,IAAI;AAClC,SAAO,iCAAiC,SAAS;AACrD;AAtES;AA4ET,SAAS,sBAAsB,SAAuC;AAClE,MAAI,CAAC,QAAS,QAAO,CAAC;AAEtB,QAAM,cAAc;AAAA,IAChB;AAAA,IAAc;AAAA,IAAe;AAAA,IAAY;AAAA,IACzC;AAAA,IAAW;AAAA,IAAe;AAAA,IAAgB;AAAA,IAC1C;AAAA,IAAa;AAAA,IAAW;AAAA,IACxB;AAAA,IAAe;AAAA,IACf;AAAA,IAAW;AAAA,IAAQ;AAAA,IAAa;AAAA,IAChC;AAAA,IAAO;AAAA,IAAY;AAAA,IACnB;AAAA,IAAY;AAAA,IAAgB;AAAA,IAC5B;AAAA,IAAW;AAAA,IAAQ;AAAA,IACnB;AAAA,IAAoB;AAAA,IAAU;AAAA,IAC9B;AAAA,IAAY;AAAA,IAAY;AAAA,IACxB;AAAA,IAAS;AAAA,IACT;AAAA,IAAiB;AAAA,IACjB;AAAA,IAAkB;AAAA,IAClB;AAAA,IAAmB;AAAA,EACvB;AAEA,QAAM,QAAQ,QAAQ,YAAY;AAClC,SAAO,YAAY,OAAO,CAAC,SAAS,MAAM,SAAS,IAAI,CAAC;AAC5D;AAtBS;AA+BT,eAAe,YAAY,OAAuC;AAC9D,QAAM,SAAS,QAAQ,IAAI;AAC3B,MAAI,CAAC,QAAQ;AACT,WAAO,MAAM,4BAA4B;AACzC,WAAO,CAAC;AAAA,EACZ;AAEA,MAAI;AACA,UAAM,WAAW,MAAM,MAAM,8CAA8C;AAAA,MACvE,QAAQ;AAAA,MACR,SAAS;AAAA,QACL,gBAAgB;AAAA,QAChB,eAAe,UAAU,MAAM;AAAA,MACnC;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACjB,OAAO;AAAA,QACP,UAAU;AAAA,UACN;AAAA,YACI,MAAM;AAAA,YACN,SACI;AAAA,UACR;AAAA,UACA;AAAA,YACI,MAAM;AAAA,YACN,SAAS;AAAA,UACb;AAAA,QACJ;AAAA,QACA,aAAa;AAAA,QACb,YAAY;AAAA,QACZ,uBAAuB;AAAA,MAC3B,CAAC;AAAA,IACL,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AACd,YAAM,YAAY,MAAM,SAAS,KAAK;AACtC,aAAO,MAAM,mBAAmB,EAAE,QAAQ,SAAS,QAAQ,MAAM,UAAU,CAAC;AAC5E,aAAO,CAAC;AAAA,IACZ;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,UAAM,UAAU,KAAK,UAAU,CAAC,GAAG,SAAS,WAAW;AAGvD,UAAM,YAAsB,CAAC;AAC7B,QAAI,MAAM,QAAQ,KAAK,SAAS,GAAG;AAC/B,iBAAW,KAAK,KAAK,WAAW;AAC5B,YAAI,OAAO,MAAM,SAAU,WAAU,KAAK,CAAC;AAAA,iBAClC,KAAK,OAAO,MAAM,YAAY,SAAS,EAAG,WAAU,KAAK,EAAE,GAAG;AAAA,MAC3E;AAAA,IACJ;AAEA,WAAO,KAAK,kBAAkB;AAAA,MAC1B,eAAe,QAAQ;AAAA,MACvB,eAAe,UAAU;AAAA,MACzB,iBAAiB,UAAU,MAAM,GAAG,CAAC;AAAA,IACzC,CAAC;AAED,QAAI,UAAU,WAAW,GAAG;AACxB,aAAO,KAAK,kEAAkE;AAC9E,aAAO,CAAC;AAAA,IACZ;AAGA,UAAM,SAAS,kBAAkB,SAAS,SAAS;AAGnD,WAAO,OACF,IAAI,CAAC,WAAW;AAAA,MACb,GAAG;AAAA,MACH,iBAAiB,YAAY,MAAM,OAAO,MAAM,SAAS,MAAM,eAAe,EAAE;AAAA,IACpF,EAAE,EACD,KAAK,CAAC,GAAG,MAAM,EAAE,kBAAkB,EAAE,eAAe;AAAA,EAC7D,SAAS,KAAK;AACV,WAAO,MAAM,uBAAuB,EAAE,OAAQ,IAAc,QAAQ,CAAC;AACrE,WAAO,CAAC;AAAA,EACZ;AACJ;AA5Ee;AAoFf,SAAS,kBAAkB,SAAiB,WAA6D;AACrG,QAAM,UAAkD,CAAC;AAGzD,QAAM,QAAQ,QAAQ,MAAM,aAAa,EAAE,OAAO,CAAC,MAAM,EAAE,KAAK,EAAE,SAAS,EAAE;AAE7E,aAAW,QAAQ,OAAO;AAEtB,QAAI,QAAQ;AACZ,QAAI,UAAU,KAAK,KAAK;AAExB,UAAM,YAAY,KAAK,MAAM,eAAe;AAC5C,QAAI,WAAW;AACX,cAAQ,UAAU,CAAC,EAAE,KAAK;AAE1B,gBAAU,KACL,QAAQ,iBAAiB,EAAE,EAC3B,QAAQ,gBAAgB,EAAE,EAC1B,KAAK;AAAA,IACd,OAAO;AAEH,YAAM,gBAAgB,KAAK,MAAM,OAAO,EAAE,CAAC;AAC3C,cAAQ,eAAe,KAAK,KAAK,KAAK,MAAM,GAAG,EAAE;AACjD,gBAAU,KAAK,MAAM,MAAM,MAAM,EAAE,KAAK;AAAA,IAC5C;AAGA,UAAM,kBAAkB,KAAK,MAAM,YAAY;AAC/C,QAAI,YAAY;AAEhB,QAAI,mBAAmB,gBAAgB,SAAS,GAAG;AAE/C,YAAM,SAAS,SAAS,gBAAgB,CAAC,EAAE,QAAQ,WAAW,EAAE,GAAG,EAAE,IAAI;AACzE,UAAI,UAAU,KAAK,SAAS,UAAU,QAAQ;AAC1C,oBAAY,UAAU,MAAM;AAAA,MAChC;AAAA,IACJ;AAGA,cAAU,QAAQ,QAAQ,YAAY,EAAE,EAAE,KAAK;AAE/C,cAAU,QAAQ,QAAQ,aAAa,EAAE,EAAE,QAAQ,aAAa,EAAE;AAElE,QAAI,CAAC,SAAS,MAAM,SAAS,EAAG;AAChC,QAAI,CAAC,UAAW;AAEhB,UAAM,SAAS,cAAc,SAAS;AAEtC,YAAQ,KAAK;AAAA,MACT,OAAO,MAAM,MAAM,GAAG,GAAG;AAAA,MACzB,SAAS,QAAQ,MAAM,GAAG,GAAG,KAAK;AAAA,MAClC,YAAY;AAAA,MACZ,eAAe;AAAA,MACf,iBAAiB,QAAQ,MAAM,GAAG,GAAG,KAAK;AAAA,IAC9C,CAAC;AAAA,EACL;AAEA,SAAO;AACX;AA1DS;AAgET,SAAS,YAAY,OAAe,SAAiB,QAAgB,QAAwB;AACzF,MAAI,QAAQ;AAGZ,MAAI,OAAO,KAAK,CAAC,MAAM,OAAO,SAAS,CAAC,CAAC,EAAG,UAAS;AAAA,WAC5C,OAAO,KAAK,CAAC,MAAM,OAAO,SAAS,CAAC,CAAC,EAAG,UAAS;AAAA,WACjD,OAAQ,UAAS;AAG1B,MAAI,MAAM,SAAS,GAAI,UAAS;AAChC,MAAI,QAAQ,SAAS,IAAK,UAAS;AACnC,MAAI,QAAQ,SAAS,IAAK,UAAS;AACnC,MAAI,MAAM,SAAS,MAAM,QAAQ,SAAS,IAAK,UAAS;AAGxD,SAAO,KAAK,IAAI,KAAK,MAAM,QAAQ,GAAG,IAAI,KAAK,EAAE;AACrD;AAhBS;AAwCT,SAAS,cAAc,KAAqB;AACxC,MAAI;AACA,WAAO,IAAI,IAAI,GAAG,EAAE,SAAS,QAAQ,QAAQ,EAAE;AAAA,EACnD,QAAQ;AACJ,WAAO;AAAA,EACX;AACJ;AANS;",
  "names": []
}
