{
  "version": 3,
  "sources": ["../../../../../../trigger/discover-research.ts"],
  "sourcesContent": ["import { schedules, logger, metadata } from \"@trigger.dev/sdk\";\nimport { createClient } from \"@supabase/supabase-js\";\n\nconst supabase = createClient(\n    process.env.SUPABASE_URL!,\n    process.env.SUPABASE_SERVICE_ROLE_KEY!\n);\n\n// ‚îÄ‚îÄ Types ‚îÄ‚îÄ\ninterface BrandKitRow {\n    user_id: string;\n    kit_name: string;\n    org_type: string;\n    org_level: string;\n    office_sought: string;\n    state: string;\n    brand_summary: string;\n}\n\ninterface StanceRow {\n    user_id: string;\n    stances: { issue: string; position: string; priority: string }[];\n}\n\ninterface SonarCitation {\n    url: string;\n    title?: string;\n}\n\ninterface SonarChoice {\n    message: {\n        content: string;\n    };\n}\n\ninterface SonarResponse {\n    choices: SonarChoice[];\n    citations?: string[] | SonarCitation[];\n}\n\ninterface ScoredTopic {\n    title: string;\n    summary: string;\n    source_url: string;\n    source_domain: string;\n    content_snippet: string;\n    relevance_score: number;\n}\n\n// ‚îÄ‚îÄ Tier-1 & Tier-2 news sources ‚îÄ‚îÄ\nconst TIER_1 = [\"nytimes.com\", \"washingtonpost.com\", \"reuters.com\", \"apnews.com\"];\nconst TIER_2 = [\"politico.com\", \"thehill.com\", \"cnn.com\", \"npr.org\", \"axios.com\", \"fec.gov\", \"opensecrets.org\", \"nbcnews.com\"];\n\n/**\n * Discover Research ‚Äî Scheduled Task (every 4 hours)\n *\n * For each user with a brand kit:\n * 1. Build a personalized search query from their stances + org info\n * 2. Call Perplexity Sonar API\n * 3. Score results (recency, source quality, relevance)\n * 4. Save top results to research_topics with suggested_by='ai'\n * 5. Deduplicate against existing topics\n */\nexport const discoverResearch = schedules.task({\n    id: \"discover-research\",\n    machine: { preset: \"small-2x\" },\n    retry: {\n        maxAttempts: 2,\n        factor: 1.5,\n        minTimeoutInMs: 5000,\n        maxTimeoutInMs: 30_000,\n    },\n    run: async () => {\n        logger.info(\"üîç Discover Research ‚Äî scheduled run starting\");\n        metadata.set(\"status\", \"fetching_users\");\n\n        // 1. Fetch all brand kits\n        const { data: brandKits, error: bkError } = await supabase\n            .from(\"brand_kits\")\n            .select(\"user_id, kit_name, org_type, org_level, office_sought, state, brand_summary\");\n\n        if (bkError || !brandKits || brandKits.length === 0) {\n            logger.info(\"No brand kits found, skipping\", { error: bkError?.message });\n            return { usersProcessed: 0, totalSaved: 0 };\n        }\n\n        logger.info(`Found ${brandKits.length} brand kits to process`);\n        metadata.set(\"totalUsers\", brandKits.length);\n\n        let totalSaved = 0;\n        let usersProcessed = 0;\n\n        for (const kit of brandKits as BrandKitRow[]) {\n            try {\n                const saved = await processUserDiscovery(kit);\n                totalSaved += saved;\n                usersProcessed++;\n                metadata.set(\"usersProcessed\", usersProcessed).set(\"totalSaved\", totalSaved);\n            } catch (err) {\n                logger.error(`Failed to process user ${kit.user_id}`, {\n                    error: (err as Error).message,\n                });\n            }\n        }\n\n        metadata.set(\"status\", \"completed\");\n        logger.info(\"üéâ Discover Research complete\", { usersProcessed, totalSaved });\n\n        return { usersProcessed, totalSaved };\n    },\n});\n\n/**\n * Process discovery for a single user\n */\nasync function processUserDiscovery(kit: BrandKitRow): Promise<number> {\n    const query = buildDiscoveryQuery(kit);\n    if (!query) {\n        logger.info(`Skipping user ${kit.user_id} ‚Äî no stances or context to search`);\n        return 0;\n    }\n\n    logger.info(`Searching for user ${kit.user_id}`, { query: query.slice(0, 100) });\n\n    // Fetch existing topic URLs for this user to deduplicate\n    const { data: existing } = await supabase\n        .from(\"research_topics\")\n        .select(\"source_url\")\n        .eq(\"user_id\", kit.user_id)\n        .order(\"created_at\", { ascending: false })\n        .limit(100);\n\n    const existingUrls = new Set((existing || []).map((t: { source_url: string }) => t.source_url));\n\n    // Call Perplexity Sonar\n    const topics = await sonarSearch(query);\n\n    if (topics.length === 0) {\n        logger.info(`No results for user ${kit.user_id}`);\n        return 0;\n    }\n\n    // Deduplicate and save top 5\n    const newTopics = topics\n        .filter((t) => !existingUrls.has(t.source_url))\n        .slice(0, 5);\n\n    let saved = 0;\n    for (const topic of newTopics) {\n        const { error } = await supabase.from(\"research_topics\").insert({\n            user_id: kit.user_id,\n            title: topic.title,\n            summary: topic.summary,\n            source_url: topic.source_url,\n            source_domain: topic.source_domain,\n            content_snippet: topic.content_snippet,\n            relevance_score: topic.relevance_score,\n            suggested_by: \"ai\",\n            used_in_draft: false,\n        });\n\n        if (!error) saved++;\n        else logger.error(\"Insert failed\", { title: topic.title, error: error.message });\n    }\n\n    logger.info(`Saved ${saved} topics for user ${kit.user_id}`);\n    return saved;\n}\n\n/**\n * Build a personalized Perplexity query from user's brand kit\n */\nfunction buildDiscoveryQuery(kit: BrandKitRow): string | null {\n    const parts: string[] = [];\n\n    // Add org context\n    if (kit.org_type === \"Candidate\") {\n        const office = kit.office_sought || \"political office\";\n        const state = kit.state ? ` in ${kit.state}` : \"\";\n        parts.push(`Latest political news relevant to a ${office} candidate${state}`);\n    } else if (kit.org_type === \"501c3\") {\n        parts.push(\"Latest nonprofit and policy news\");\n    } else if (kit.org_type === \"501c4\") {\n        parts.push(\"Latest political advocacy and policy news\");\n    } else if (kit.org_type) {\n        parts.push(\"Latest political and campaign news\");\n    }\n\n    // Add brand summary context\n    if (kit.brand_summary) {\n        const summary = kit.brand_summary.slice(0, 200);\n        parts.push(`related to: ${summary}`);\n    }\n\n    // If we have nothing to search for, skip\n    if (parts.length === 0) return null;\n\n    // Add recency emphasis\n    parts.push(\"Focus on the past 48 hours. Include fundraising angles and donor-relevant stories.\");\n\n    return parts.join(\". \");\n}\n\n/**\n * Call Perplexity Sonar API (chat completions endpoint)\n */\nasync function sonarSearch(query: string): Promise<ScoredTopic[]> {\n    const apiKey = process.env.PERPLEXITY_API_KEY;\n    if (!apiKey) {\n        logger.error(\"PERPLEXITY_API_KEY not set\");\n        return [];\n    }\n\n    try {\n        const response = await fetch(\"https://api.perplexity.ai/chat/completions\", {\n            method: \"POST\",\n            headers: {\n                \"Content-Type\": \"application/json\",\n                Authorization: `Bearer ${apiKey}`,\n            },\n            body: JSON.stringify({\n                model: \"sonar\",\n                messages: [\n                    {\n                        role: \"system\",\n                        content:\n                            \"You are a political research assistant. Return ONLY a JSON array of objects with keys: title, summary, source_url. Each item should be a distinct news story or policy development. Return 5-10 items, sorted by relevance. No markdown, no explanation, just the JSON array.\",\n                    },\n                    {\n                        role: \"user\",\n                        content: query,\n                    },\n                ],\n                temperature: 0.1,\n                max_tokens: 2000,\n                search_recency_filter: \"week\",\n            }),\n        });\n\n        if (!response.ok) {\n            const errorText = await response.text();\n            logger.error(\"Sonar API error\", { status: response.status, body: errorText });\n            return [];\n        }\n\n        const data = (await response.json()) as SonarResponse;\n        const content = data.choices?.[0]?.message?.content || \"\";\n\n        // Parse JSON from response\n        const parsed = parseJsonArray(content);\n        if (!parsed || parsed.length === 0) {\n            logger.warn(\"Could not parse Sonar response\", { content: content.slice(0, 200) });\n            return [];\n        }\n\n        // Score and return\n        return parsed\n            .filter((item: Record<string, string>) => item.title && item.summary)\n            .map((item: Record<string, string>) => {\n                const domain = extractDomain(item.source_url || \"\");\n                const score = scoreResult(item.title, item.summary, domain, query);\n\n                return {\n                    title: item.title,\n                    summary: item.summary,\n                    source_url: item.source_url || \"\",\n                    source_domain: domain,\n                    content_snippet: (item.summary || \"\").slice(0, 300),\n                    relevance_score: score,\n                };\n            })\n            .sort((a: ScoredTopic, b: ScoredTopic) => b.relevance_score - a.relevance_score);\n    } catch (err) {\n        logger.error(\"Sonar search failed\", { error: (err as Error).message });\n        return [];\n    }\n}\n\n/**\n * Score a result on a fixed 0-10 scale.\n * Source quality is the primary signal ‚Äî Sonar already filtered for relevance.\n */\nfunction scoreResult(title: string, summary: string, domain: string, _query: string): number {\n    let score = 2; // Base score: Sonar returned it, so it's at least somewhat relevant\n\n    // Source quality (0-5 points) ‚Äî biggest differentiator\n    if (TIER_1.some((d) => domain.includes(d))) score += 5;\n    else if (TIER_2.some((d) => domain.includes(d))) score += 3;\n    else if (domain) score += 1; // Known domain but not in our tiers\n\n    // Content richness bonus (0-2 points)\n    if (title.length > 30) score += 0.5;\n    if (summary.length > 100) score += 0.5;\n    if (summary.length > 200) score += 0.5;\n    if (title.length > 50 && summary.length > 150) score += 0.5;\n\n    // Cap at 10\n    return Math.min(Math.round(score * 100) / 100, 10);\n}\n\n/**\n * Parse a JSON array from potentially messy LLM output\n */\nfunction parseJsonArray(content: string): Record<string, string>[] | null {\n    try {\n        // Try direct parse first\n        const parsed = JSON.parse(content);\n        if (Array.isArray(parsed)) return parsed;\n    } catch {\n        // Try extracting JSON from markdown code blocks\n        const match = content.match(/\\[[\\s\\S]*\\]/);\n        if (match) {\n            try {\n                return JSON.parse(match[0]);\n            } catch {\n                return null;\n            }\n        }\n    }\n    return null;\n}\n\nfunction extractDomain(url: string): string {\n    try {\n        return new URL(url).hostname.replace(\"www.\", \"\");\n    } catch {\n        return \"\";\n    }\n}\n"],
  "mappings": ";;;;;;;;;;;;;;;;AAAA;AAGA,IAAM,WAAW;AAAA,EACb,QAAQ,IAAI;AAAA,EACZ,QAAQ,IAAI;AAChB;AA4CA,IAAM,SAAS,CAAC,eAAe,sBAAsB,eAAe,YAAY;AAChF,IAAM,SAAS,CAAC,gBAAgB,eAAe,WAAW,WAAW,aAAa,WAAW,mBAAmB,aAAa;AAYtH,IAAM,mBAAmB,kBAAU,KAAK;AAAA,EAC3C,IAAI;AAAA,EACJ,SAAS,EAAE,QAAQ,WAAW;AAAA,EAC9B,OAAO;AAAA,IACH,aAAa;AAAA,IACb,QAAQ;AAAA,IACR,gBAAgB;AAAA,IAChB,gBAAgB;AAAA,EACpB;AAAA,EACA,KAAK,mCAAY;AACb,WAAO,KAAK,+CAA+C;AAC3D,aAAS,IAAI,UAAU,gBAAgB;AAGvC,UAAM,EAAE,MAAM,WAAW,OAAO,QAAQ,IAAI,MAAM,SAC7C,KAAK,YAAY,EACjB,OAAO,6EAA6E;AAEzF,QAAI,WAAW,CAAC,aAAa,UAAU,WAAW,GAAG;AACjD,aAAO,KAAK,iCAAiC,EAAE,OAAO,SAAS,QAAQ,CAAC;AACxE,aAAO,EAAE,gBAAgB,GAAG,YAAY,EAAE;AAAA,IAC9C;AAEA,WAAO,KAAK,SAAS,UAAU,MAAM,wBAAwB;AAC7D,aAAS,IAAI,cAAc,UAAU,MAAM;AAE3C,QAAI,aAAa;AACjB,QAAI,iBAAiB;AAErB,eAAW,OAAO,WAA4B;AAC1C,UAAI;AACA,cAAM,QAAQ,MAAM,qBAAqB,GAAG;AAC5C,sBAAc;AACd;AACA,iBAAS,IAAI,kBAAkB,cAAc,EAAE,IAAI,cAAc,UAAU;AAAA,MAC/E,SAAS,KAAK;AACV,eAAO,MAAM,0BAA0B,IAAI,OAAO,IAAI;AAAA,UAClD,OAAQ,IAAc;AAAA,QAC1B,CAAC;AAAA,MACL;AAAA,IACJ;AAEA,aAAS,IAAI,UAAU,WAAW;AAClC,WAAO,KAAK,iCAAiC,EAAE,gBAAgB,WAAW,CAAC;AAE3E,WAAO,EAAE,gBAAgB,WAAW;AAAA,EACxC,GArCK;AAsCT,CAAC;AAKD,eAAe,qBAAqB,KAAmC;AACnE,QAAM,QAAQ,oBAAoB,GAAG;AACrC,MAAI,CAAC,OAAO;AACR,WAAO,KAAK,iBAAiB,IAAI,OAAO,oCAAoC;AAC5E,WAAO;AAAA,EACX;AAEA,SAAO,KAAK,sBAAsB,IAAI,OAAO,IAAI,EAAE,OAAO,MAAM,MAAM,GAAG,GAAG,EAAE,CAAC;AAG/E,QAAM,EAAE,MAAM,SAAS,IAAI,MAAM,SAC5B,KAAK,iBAAiB,EACtB,OAAO,YAAY,EACnB,GAAG,WAAW,IAAI,OAAO,EACzB,MAAM,cAAc,EAAE,WAAW,MAAM,CAAC,EACxC,MAAM,GAAG;AAEd,QAAM,eAAe,IAAI,KAAK,YAAY,CAAC,GAAG,IAAI,CAAC,MAA8B,EAAE,UAAU,CAAC;AAG9F,QAAM,SAAS,MAAM,YAAY,KAAK;AAEtC,MAAI,OAAO,WAAW,GAAG;AACrB,WAAO,KAAK,uBAAuB,IAAI,OAAO,EAAE;AAChD,WAAO;AAAA,EACX;AAGA,QAAM,YAAY,OACb,OAAO,CAAC,MAAM,CAAC,aAAa,IAAI,EAAE,UAAU,CAAC,EAC7C,MAAM,GAAG,CAAC;AAEf,MAAI,QAAQ;AACZ,aAAW,SAAS,WAAW;AAC3B,UAAM,EAAE,MAAM,IAAI,MAAM,SAAS,KAAK,iBAAiB,EAAE,OAAO;AAAA,MAC5D,SAAS,IAAI;AAAA,MACb,OAAO,MAAM;AAAA,MACb,SAAS,MAAM;AAAA,MACf,YAAY,MAAM;AAAA,MAClB,eAAe,MAAM;AAAA,MACrB,iBAAiB,MAAM;AAAA,MACvB,iBAAiB,MAAM;AAAA,MACvB,cAAc;AAAA,MACd,eAAe;AAAA,IACnB,CAAC;AAED,QAAI,CAAC,MAAO;AAAA,QACP,QAAO,MAAM,iBAAiB,EAAE,OAAO,MAAM,OAAO,OAAO,MAAM,QAAQ,CAAC;AAAA,EACnF;AAEA,SAAO,KAAK,SAAS,KAAK,oBAAoB,IAAI,OAAO,EAAE;AAC3D,SAAO;AACX;AApDe;AAyDf,SAAS,oBAAoB,KAAiC;AAC1D,QAAM,QAAkB,CAAC;AAGzB,MAAI,IAAI,aAAa,aAAa;AAC9B,UAAM,SAAS,IAAI,iBAAiB;AACpC,UAAM,QAAQ,IAAI,QAAQ,OAAO,IAAI,KAAK,KAAK;AAC/C,UAAM,KAAK,uCAAuC,MAAM,aAAa,KAAK,EAAE;AAAA,EAChF,WAAW,IAAI,aAAa,SAAS;AACjC,UAAM,KAAK,kCAAkC;AAAA,EACjD,WAAW,IAAI,aAAa,SAAS;AACjC,UAAM,KAAK,2CAA2C;AAAA,EAC1D,WAAW,IAAI,UAAU;AACrB,UAAM,KAAK,oCAAoC;AAAA,EACnD;AAGA,MAAI,IAAI,eAAe;AACnB,UAAM,UAAU,IAAI,cAAc,MAAM,GAAG,GAAG;AAC9C,UAAM,KAAK,eAAe,OAAO,EAAE;AAAA,EACvC;AAGA,MAAI,MAAM,WAAW,EAAG,QAAO;AAG/B,QAAM,KAAK,oFAAoF;AAE/F,SAAO,MAAM,KAAK,IAAI;AAC1B;AA7BS;AAkCT,eAAe,YAAY,OAAuC;AAC9D,QAAM,SAAS,QAAQ,IAAI;AAC3B,MAAI,CAAC,QAAQ;AACT,WAAO,MAAM,4BAA4B;AACzC,WAAO,CAAC;AAAA,EACZ;AAEA,MAAI;AACA,UAAM,WAAW,MAAM,MAAM,8CAA8C;AAAA,MACvE,QAAQ;AAAA,MACR,SAAS;AAAA,QACL,gBAAgB;AAAA,QAChB,eAAe,UAAU,MAAM;AAAA,MACnC;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACjB,OAAO;AAAA,QACP,UAAU;AAAA,UACN;AAAA,YACI,MAAM;AAAA,YACN,SACI;AAAA,UACR;AAAA,UACA;AAAA,YACI,MAAM;AAAA,YACN,SAAS;AAAA,UACb;AAAA,QACJ;AAAA,QACA,aAAa;AAAA,QACb,YAAY;AAAA,QACZ,uBAAuB;AAAA,MAC3B,CAAC;AAAA,IACL,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AACd,YAAM,YAAY,MAAM,SAAS,KAAK;AACtC,aAAO,MAAM,mBAAmB,EAAE,QAAQ,SAAS,QAAQ,MAAM,UAAU,CAAC;AAC5E,aAAO,CAAC;AAAA,IACZ;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,UAAM,UAAU,KAAK,UAAU,CAAC,GAAG,SAAS,WAAW;AAGvD,UAAM,SAAS,eAAe,OAAO;AACrC,QAAI,CAAC,UAAU,OAAO,WAAW,GAAG;AAChC,aAAO,KAAK,kCAAkC,EAAE,SAAS,QAAQ,MAAM,GAAG,GAAG,EAAE,CAAC;AAChF,aAAO,CAAC;AAAA,IACZ;AAGA,WAAO,OACF,OAAO,CAAC,SAAiC,KAAK,SAAS,KAAK,OAAO,EACnE,IAAI,CAAC,SAAiC;AACnC,YAAM,SAAS,cAAc,KAAK,cAAc,EAAE;AAClD,YAAM,QAAQ,YAAY,KAAK,OAAO,KAAK,SAAS,QAAQ,KAAK;AAEjE,aAAO;AAAA,QACH,OAAO,KAAK;AAAA,QACZ,SAAS,KAAK;AAAA,QACd,YAAY,KAAK,cAAc;AAAA,QAC/B,eAAe;AAAA,QACf,kBAAkB,KAAK,WAAW,IAAI,MAAM,GAAG,GAAG;AAAA,QAClD,iBAAiB;AAAA,MACrB;AAAA,IACJ,CAAC,EACA,KAAK,CAAC,GAAgB,MAAmB,EAAE,kBAAkB,EAAE,eAAe;AAAA,EACvF,SAAS,KAAK;AACV,WAAO,MAAM,uBAAuB,EAAE,OAAQ,IAAc,QAAQ,CAAC;AACrE,WAAO,CAAC;AAAA,EACZ;AACJ;AAtEe;AA4Ef,SAAS,YAAY,OAAe,SAAiB,QAAgB,QAAwB;AACzF,MAAI,QAAQ;AAGZ,MAAI,OAAO,KAAK,CAAC,MAAM,OAAO,SAAS,CAAC,CAAC,EAAG,UAAS;AAAA,WAC5C,OAAO,KAAK,CAAC,MAAM,OAAO,SAAS,CAAC,CAAC,EAAG,UAAS;AAAA,WACjD,OAAQ,UAAS;AAG1B,MAAI,MAAM,SAAS,GAAI,UAAS;AAChC,MAAI,QAAQ,SAAS,IAAK,UAAS;AACnC,MAAI,QAAQ,SAAS,IAAK,UAAS;AACnC,MAAI,MAAM,SAAS,MAAM,QAAQ,SAAS,IAAK,UAAS;AAGxD,SAAO,KAAK,IAAI,KAAK,MAAM,QAAQ,GAAG,IAAI,KAAK,EAAE;AACrD;AAhBS;AAqBT,SAAS,eAAe,SAAkD;AACtE,MAAI;AAEA,UAAM,SAAS,KAAK,MAAM,OAAO;AACjC,QAAI,MAAM,QAAQ,MAAM,EAAG,QAAO;AAAA,EACtC,QAAQ;AAEJ,UAAM,QAAQ,QAAQ,MAAM,aAAa;AACzC,QAAI,OAAO;AACP,UAAI;AACA,eAAO,KAAK,MAAM,MAAM,CAAC,CAAC;AAAA,MAC9B,QAAQ;AACJ,eAAO;AAAA,MACX;AAAA,IACJ;AAAA,EACJ;AACA,SAAO;AACX;AAjBS;AAmBT,SAAS,cAAc,KAAqB;AACxC,MAAI;AACA,WAAO,IAAI,IAAI,GAAG,EAAE,SAAS,QAAQ,QAAQ,EAAE;AAAA,EACnD,QAAQ;AACJ,WAAO;AAAA,EACX;AACJ;AANS;",
  "names": []
}
